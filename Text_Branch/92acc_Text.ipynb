{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZSTT_eI3nVu",
        "colab_type": "code",
        "outputId": "59594dff-0a7d-4d4f-d4ac-f985bc3b4067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "#importing libraries\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Activation\n",
        "from keras.layers import Flatten, BatchNormalization, Concatenate, add\n",
        "from keras.layers import Embedding, Dropout, Conv1D, MaxPooling1D, Conv2D\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers.merge import concatenate\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "#Loading the data in a dataframe\n",
        "df = pd.read_csv('drive/My Drive/The_Research/all_data_refined.csv')\n",
        "df = df.drop(['emotion'], axis = 1)\n",
        "docs = df['text']\n",
        "\n",
        "#test\n",
        "for i in range(len(docs)):\n",
        "  if docs[i] == \"\": print(i)\n",
        "\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "\n",
        "# pad documents to a max length\n",
        "wordlen = max(df['word_count'])\n",
        "max_length = wordlen # Change this if needed\n",
        "print(\"Max_Length: \", max_length)\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('drive/My Drive/glove_data/glove.6B/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "#load the labels\n",
        "type = df['type']\n",
        "labels = []\n",
        "for types in type:\n",
        "  if types == 'real':\n",
        "    labels.append(1)\n",
        "  elif types == 'fake':\n",
        "    labels.append(0)\n",
        "\n",
        "\n",
        "arr = df[df.columns[8: 23]]\n",
        "\n",
        "listt = list()\n",
        "for i in range(len(df)):\n",
        "  listt.append(i)\n",
        "\n",
        "\n",
        "X_train_1 = list()\n",
        "X_train_2 = list()\n",
        "X_test_1 = list()\n",
        "X_test_2 = list()\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(listt, labels, test_size=0.33)\n",
        "\n",
        "for i in X_train:\n",
        "  X_train_1.append(padded_docs[i])\n",
        "  X_train_2.append(arr.iloc[i])\n",
        "\n",
        "for i in X_test:\n",
        "  X_test_1.append(padded_docs[i])\n",
        "  X_test_2.append(arr.iloc[i])\n",
        "\n",
        "\n",
        "# define Implicit\n",
        "inputs1 = Input(shape=(max_length,))\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)(inputs1)\n",
        "a = (Dropout(0.5))(e)\n",
        "b = (Conv1D(filters=10, kernel_size=(4)))(a)\n",
        "c=MaxPooling1D(pool_size=2)(b)\n",
        "d=Flatten()(c)\n",
        "f=Dense(128)(d)\n",
        "g=(BatchNormalization())(f)\n",
        "z = Activation('relu')\n",
        "h=(Dropout(0.8))(g)\n",
        "\n",
        "# define Explicit\n",
        "inputs2 = Input(shape=(15,))\n",
        "q=(Dense(128))(inputs2)\n",
        "r=(BatchNormalization())(q)\n",
        "u=Activation('relu')(r)\n",
        "\n",
        "print(\"h: \", h)\n",
        "print(\"u: \", u)\n",
        "\n",
        "merged = concatenate([h, u])\n",
        "dense1 = Dense(10, activation='relu')(merged)\n",
        "outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "print(\"____________________\")\n",
        "print(model.summary())\n",
        "print(\"____________________\")\n",
        "\n",
        "plot_model(model, show_shapes=True, to_file='multichannelbeta.png')\n",
        "\n",
        "# fit the model\n",
        "print(\"Fitting\")\n",
        "model.fit([X_train_1, X_train_2], array(y_train), epochs=10, verbose=1, batch_size=16)\n",
        "print(\"Fitted\")\n",
        "\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"textModel.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"textModel.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "print(\"____________________\")\n",
        "loss, accuracy = model.evaluate([X_test_1, X_test_2] , array(y_test), verbose=1, batch_size=16)\n",
        "print('Accuracy: %f' % (accuracy*100))\n",
        "\n",
        "print(\"____________________\")\n",
        "output = model.predict([X_test_1, X_test_2])\n",
        "print(output)\n",
        "\n",
        "\n",
        "def finalOutputWithDelta(delta = 0.0):\n",
        "  for i in range(len(output)):\n",
        "    if output[i] >= (0.5 + delta):\n",
        "      output[i] = 1\n",
        "    elif output[i] < (0.5 + delta):\n",
        "      output[i] = 0\n",
        "  error = 0\n",
        "  correct = 0\n",
        "  for i in range(len(output)):\n",
        "    error = error + ((output[i] - y_test[i]) ** 2)\n",
        "    if output[i] == y_test[i]: correct+=1\n",
        "\n",
        "  print(\"error: \", error)\n",
        "  print(output)\n",
        "  print(correct/6605)\n",
        "finalOutputWithDelta()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Max_Length:  24177\n",
            "h:  Tensor(\"dropout_2/cond/Identity:0\", shape=(None, 128), dtype=float32)\n",
            "u:  Tensor(\"activation_2/Relu:0\", shape=(None, 128), dtype=float32)\n",
            "____________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 24177)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 24177, 100)   16347600    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 24177, 100)   0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 24174, 10)    4010        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 12087, 10)    0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 120870)       0           max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 15)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          15471488    flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          2048        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 128)          512         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 128)          0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 128)          0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 256)          0           dropout_2[0][0]                  \n",
            "                                                                 activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           2570        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 31,828,751\n",
            "Trainable params: 15,480,639\n",
            "Non-trainable params: 16,348,112\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "____________________\n",
            "Fitting\n",
            "Epoch 1/10\n",
            "13410/13410 [==============================] - 941s 70ms/step - loss: 0.4822 - accuracy: 0.7803\n",
            "Epoch 2/10\n",
            "13410/13410 [==============================] - 934s 70ms/step - loss: 0.3723 - accuracy: 0.8417\n",
            "Epoch 3/10\n",
            "13410/13410 [==============================] - 933s 70ms/step - loss: 0.3338 - accuracy: 0.8559\n",
            "Epoch 4/10\n",
            "13410/13410 [==============================] - 936s 70ms/step - loss: 0.3238 - accuracy: 0.8591\n",
            "Epoch 5/10\n",
            "13410/13410 [==============================] - 933s 70ms/step - loss: 0.2964 - accuracy: 0.8729\n",
            "Epoch 6/10\n",
            "13410/13410 [==============================] - 927s 69ms/step - loss: 0.2862 - accuracy: 0.8794\n",
            "Epoch 7/10\n",
            "13410/13410 [==============================] - 935s 70ms/step - loss: 0.2666 - accuracy: 0.8894\n",
            "Epoch 8/10\n",
            "13410/13410 [==============================] - 975s 73ms/step - loss: 0.2553 - accuracy: 0.8930\n",
            "Epoch 9/10\n",
            "13410/13410 [==============================] - 949s 71ms/step - loss: 0.2483 - accuracy: 0.8988\n",
            "Epoch 10/10\n",
            "13410/13410 [==============================] - 978s 73ms/step - loss: 0.2351 - accuracy: 0.9025\n",
            "Fitted\n",
            "Saved model to disk\n",
            "____________________\n",
            "6605/6605 [==============================] - 81s 12ms/step\n",
            "Accuracy: 91.688114\n",
            "____________________\n",
            "[[0.9181937 ]\n",
            " [0.23864925]\n",
            " [0.1110394 ]\n",
            " ...\n",
            " [0.9391237 ]\n",
            " [0.93599904]\n",
            " [0.05183615]]\n",
            "error:  [549.]\n",
            "[[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "0.9168811506434519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xpocFDZ_ztd",
        "colab_type": "code",
        "outputId": "3a699129-067b-4aa8-9293-a873076e85f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Loading the model:\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_json\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('textModel.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"textModel.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics= ['accuracy', 'mse'])\n",
        "score = loaded_model.evaluate([X_test_1, X_test_2] , array(y_test), verbose=1)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "6605/6605 [==============================] - 1s 82us/step\n",
            "accuracy: 87.57%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9XfaGP2A81l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c1ec9f03-baea-41b7-9336-0e414b702296"
      },
      "source": [
        "arr.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_count</th>\n",
              "      <th>average_word_count</th>\n",
              "      <th>exclamation_count</th>\n",
              "      <th>capital_count</th>\n",
              "      <th>question_count</th>\n",
              "      <th>negation_count</th>\n",
              "      <th>fpp_count</th>\n",
              "      <th>capital_words</th>\n",
              "      <th>anger</th>\n",
              "      <th>disgust</th>\n",
              "      <th>fear</th>\n",
              "      <th>joy</th>\n",
              "      <th>sadness</th>\n",
              "      <th>surprise</th>\n",
              "      <th>trust</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1082</td>\n",
              "      <td>12.729412</td>\n",
              "      <td>0</td>\n",
              "      <td>230</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.004245</td>\n",
              "      <td>0.002355</td>\n",
              "      <td>0.014836</td>\n",
              "      <td>0.408354</td>\n",
              "      <td>0.033534</td>\n",
              "      <td>0.408217</td>\n",
              "      <td>0.080312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>344</td>\n",
              "      <td>10.117647</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.025900</td>\n",
              "      <td>0.002831</td>\n",
              "      <td>0.063273</td>\n",
              "      <td>0.306524</td>\n",
              "      <td>0.019827</td>\n",
              "      <td>0.326676</td>\n",
              "      <td>0.102390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1090</td>\n",
              "      <td>16.268657</td>\n",
              "      <td>1</td>\n",
              "      <td>189</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.019803</td>\n",
              "      <td>0.010692</td>\n",
              "      <td>0.024147</td>\n",
              "      <td>0.580804</td>\n",
              "      <td>0.083064</td>\n",
              "      <td>0.219682</td>\n",
              "      <td>0.040171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1302</td>\n",
              "      <td>13.151515</td>\n",
              "      <td>0</td>\n",
              "      <td>283</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>0.010901</td>\n",
              "      <td>0.001291</td>\n",
              "      <td>0.023162</td>\n",
              "      <td>0.688297</td>\n",
              "      <td>0.004354</td>\n",
              "      <td>0.086323</td>\n",
              "      <td>0.117578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>518</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>115</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.020370</td>\n",
              "      <td>0.002589</td>\n",
              "      <td>0.040738</td>\n",
              "      <td>0.293498</td>\n",
              "      <td>0.016402</td>\n",
              "      <td>0.016180</td>\n",
              "      <td>0.609104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_count  average_word_count  ...  surprise     trust\n",
              "0        1082           12.729412  ...  0.408217  0.080312\n",
              "1         344           10.117647  ...  0.326676  0.102390\n",
              "2        1090           16.268657  ...  0.219682  0.040171\n",
              "3        1302           13.151515  ...  0.086323  0.117578\n",
              "4         518           14.000000  ...  0.016180  0.609104\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO0a5qUhQHhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "a38a6e15-b435-4e32-8036-9c2b0c5fb9e2"
      },
      "source": [
        "arr.iloc[2]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "word_count            1090.000000\n",
              "average_word_count      16.268657\n",
              "exclamation_count        1.000000\n",
              "capital_count          189.000000\n",
              "question_count           1.000000\n",
              "negation_count           0.000000\n",
              "fpp_count                2.000000\n",
              "capital_words            3.000000\n",
              "anger                    0.019803\n",
              "disgust                  0.010692\n",
              "fear                     0.024147\n",
              "joy                      0.580804\n",
              "sadness                  0.083064\n",
              "surprise                 0.219682\n",
              "trust                    0.040171\n",
              "Name: 2, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQAaVPr2RfBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "274dc2d0-952e-42a3-fcfb-93a407150aad"
      },
      "source": [
        "count = 0\n",
        "for i in range(len(X_test_2)):\n",
        "  for j in range(len(X_train_2)):\n",
        "    if X_test_2[i].name == X_train_2[j].name: \n",
        "      print(\"Test: \", i, \"Train: \", j)\n",
        "    else: \n",
        "      count +=1\n",
        "\n",
        "print(count)\n",
        "\n",
        "print(X_test == X_train)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100150056\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHja63VISVU3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3838666c-937f-47cf-a042-e874ac5e4190"
      },
      "source": [
        "X_train_2[0].name"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3198"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOYBxkoTVWnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d647314f-fd6f-4676-a88c-3903723d80ae"
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtp-e3SOz7ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m2 = Model()\n",
        "m2.load\n",
        "m2.plot_model(m2, show_shapes=True, to_file='calssifier_news.png')\n",
        "plot_model(model, show_shapes=True, to_file='multichannelbeta.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}